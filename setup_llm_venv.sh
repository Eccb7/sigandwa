#!/bin/bash

echo "ðŸš€ Setting up Sigandwa Local LLM (CPU-optimized)..."
echo ""

# Navigate to backend
cd backend

# Check if virtual environment exists
if [ ! -d "venv" ]; then
    echo "ðŸ“¦ Creating Python virtual environment..."
    python3 -m venv venv
    echo "âœ… Virtual environment created"
fi

# Activate virtual environment
echo "ðŸ”§ Activating virtual environment..."
source venv/bin/activate

# Install dependencies
echo "ðŸ“¥ Installing LLM dependencies (CPU-only)..."
pip install --upgrade pip
pip install llama-cpp-python huggingface-hub

# Create models directory
mkdir -p models

echo ""
echo "âœ… Setup complete!"
echo ""
echo "ðŸ“¥ The model (Phi-2, ~1.5GB) will be automatically downloaded on first API call"
echo ""
echo "ðŸš€ To start the backend:"
echo "   cd backend"
echo "   source venv/bin/activate"
echo "   python -m uvicorn app.main:app --reload"
echo ""
echo "ðŸ” Check model status:"
echo "   curl http://localhost:8000/api/v1/llm/model-info"
echo ""
echo "ðŸ’¬ The chat interface will appear as a floating button on the frontend"
echo ""
echo "âš™ï¸  System Info:"
echo "   CPU cores: $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 'unknown')"
echo "   Model: Phi-2 (2.7B parameters, 4-bit quantized)"
echo "   Expected speed: 40-60 tokens/second on modern CPU"
echo ""
